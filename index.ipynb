{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the input file\n",
    "input_file_path = 'Input.xlsx'  # Update this path to your local file\n",
    "input_df = pd.read_excel(input_file_path)\n",
    "\n",
    "# Function to extract article text from specific classes\n",
    "def extract_article_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract title with exact class 'entry-title'\n",
    "    title_element = soup.find(lambda tag: tag.get('class') == ['entry-title'])\n",
    "    title = title_element.get_text() if title_element else ''\n",
    "    \n",
    "    # Extract content with exact class combination 'td-post-content tagdiv-type'\n",
    "    content_element = soup.find(lambda tag: tag.get('class') == ['td-post-content', 'tagdiv-type'])\n",
    "    content = content_element.get_text() if content_element else ''\n",
    "    \n",
    "    return title, content\n",
    "\n",
    "# Directory to save the extracted text files\n",
    "output_text_dir = 'path_to_save_extracted_articles'  # Update this path to your desired directory\n",
    "os.makedirs(output_text_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each URL in the input file and extract text\n",
    "for index, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    try:\n",
    "        title, article_text = extract_article_text(url)\n",
    "        # Save the article text to a file named with URL_ID\n",
    "        with open(os.path.join(output_text_dir, f'{url_id}.txt'), 'w', encoding='utf-8') as file:\n",
    "            file.write(title + '\\n' + article_text)\n",
    "        print(f'Successfully extracted and saved article for URL_ID: {url_id}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to extract article for URL_ID: {url_id}. Error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12919, 2006, 4783)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = []\n",
    "# Load stop words\n",
    "def load_stop_words(file_path):\n",
    "    encodings = ['utf-8', 'iso-8859-1', 'latin-1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                stop_words = file.read().splitlines()\n",
    "            return stop_words\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise Exception(f\"Failed to decode {file_path} with available encodings.\")\n",
    "\n",
    "stop_words_files = [\n",
    "    'StopWords/StopWords_Currencies.txt',\n",
    "    'StopWords/StopWords_DatesandNumbers.txt',\n",
    "    'StopWords/StopWords_Auditor.txt',\n",
    "    'StopWords/StopWords_Generic.txt',\n",
    "    'StopWords/StopWords_GenericLong.txt',\n",
    "    'StopWords/StopWords_Geographic.txt',\n",
    "    'StopWords/StopWords_Names.txt'\n",
    "]\n",
    "\n",
    "stop_words = []\n",
    "for file_path in stop_words_files:\n",
    "    stop_words.extend(load_stop_words(file_path))\n",
    "\n",
    "# Load positive and negative words\n",
    "positive_words_file = 'MasterDictionary/positive-words.txt'\n",
    "negative_words_file = 'MasterDictionary/negative-words.txt'\n",
    "\n",
    "positive_words = load_stop_words(positive_words_file)\n",
    "negative_words = load_stop_words(negative_words_file)\n",
    "\n",
    "stop_words = set(stop_words)\n",
    "positive_words = set(positive_words)\n",
    "negative_words = set(negative_words)\n",
    "\n",
    "# Display the counts of loaded words\n",
    "len(stop_words), len(positive_words), len(negative_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MUZAMIL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute text analysis\n",
    "def compute_text_analysis(article_text):\n",
    "    # Tokenize sentences and words\n",
    "    sentences = sent_tokenize(article_text)\n",
    "    words = word_tokenize(article_text.lower())\n",
    "    \n",
    "    # Remove stop words and non-alphanumeric tokens\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Positive, negative, polarity, and subjectivity scores using the provided word lists\n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 1e-6)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(words) + 1e-6)\n",
    "    \n",
    "    # Average sentence length\n",
    "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
    "    \n",
    "    # Complex words (words with more than two syllables)\n",
    "    complex_words = [word for word in words if len(re.findall(r'[aeiouyAEIOUY]', word)) > 2]\n",
    "    complex_word_count = len(complex_words)\n",
    "    \n",
    "    # Percentage of complex words\n",
    "    percentage_of_complex_words = (complex_word_count / len(words)) * 100\n",
    "    \n",
    "    # FOG index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
    "    \n",
    "    # Average number of words per sentence\n",
    "    avg_number_of_words_per_sentence = len(words) / len(sentences)\n",
    "    \n",
    "    # Word count\n",
    "    word_count = len(words)\n",
    "    \n",
    "    # Syllables per word\n",
    "    syllable_per_word = sum(len(re.findall(r'[aeiouyAEIOUY]', word)) for word in words) / len(words)\n",
    "    \n",
    "    # Personal pronouns count\n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', article_text, re.I))\n",
    "    \n",
    "    # Average word length\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    return {\n",
    "        \"POSITIVE SCORE\": positive_score,\n",
    "        \"NEGATIVE SCORE\": negative_score,\n",
    "        \"POLARITY SCORE\": polarity_score,\n",
    "        \"SUBJECTIVITY SCORE\": subjectivity_score,\n",
    "        \"AVG SENTENCE LENGTH\": avg_sentence_length,\n",
    "        \"PERCENTAGE OF COMPLEX WORDS\": percentage_of_complex_words,\n",
    "        \"FOG INDEX\": fog_index,\n",
    "        \"AVG NUMBER OF WORDS PER SENTENCE\": avg_number_of_words_per_sentence,\n",
    "        \"COMPLEX WORD COUNT\": complex_word_count,\n",
    "        \"WORD COUNT\": word_count,\n",
    "        \"SYLLABLE PER WORD\": syllable_per_word,\n",
    "        \"PERSONAL PRONOUNS\": personal_pronouns,\n",
    "        \"AVG WORD LENGTH\": avg_word_length,\n",
    "    }\n",
    "\n",
    "# Process each extracted article and compute the text analysis\n",
    "extraction_dir = 'path_to_save_extracted_articles'\n",
    "extracted_files = os.listdir(extraction_dir)\n",
    "\n",
    "results = []\n",
    "\n",
    "for file_name in extracted_files:\n",
    "    file_path = os.path.join(extraction_dir, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        article_text = file.read()\n",
    "    \n",
    "    analysis_results = compute_text_analysis(article_text)\n",
    "    url_id = os.path.splitext(file_name)[0]\n",
    "    analysis_results[\"URL_ID\"] = url_id\n",
    "    results.append(analysis_results)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "output_file_path = 'Output.csv'\n",
    "results_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
